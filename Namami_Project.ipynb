{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namami-13/Twitter-Sentiment-Analyzer/blob/master/Namami_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THlLhSTAiVp5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnjPcYh-tNu3",
        "outputId": "3cdaf173-e74e-4d2d-e7bb-71d342f65d91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtvxT14_e5JJ"
      },
      "outputs": [],
      "source": [
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr1bg8-DqEup",
        "outputId": "4c518d19-e6bf-4d42-cef9-8c5946e60329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File not found. Downloading...\n",
            "--2025-09-01 16:24:19--  http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip [following]\n",
            "--2025-09-01 16:24:19--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81363704 (78M) [application/zip]\n",
            "Saving to: ‚Äòtrainingandtestdata.zip‚Äô\n",
            "\n",
            "trainingandtestdata 100%[===================>]  77.59M  48.7MB/s    in 1.6s    \n",
            "\n",
            "2025-09-01 16:24:21 (48.7 MB/s) - ‚Äòtrainingandtestdata.zip‚Äô saved [81363704/81363704]\n",
            "\n",
            "Archive:  trainingandtestdata.zip\n",
            "  inflating: testdata.manual.2009.06.14.csv  \n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Verify if file exists and check size\n",
        "file_path = 'training.1600000.processed.noemoticon.csv'\n",
        "if os.path.exists(file_path):\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    print(f\"File exists. Size: {file_size/1024/1024:.2f} MB\")\n",
        "    # Expected size should be around 80MB for the full dataset\n",
        "else:\n",
        "    print(\"File not found. Downloading...\")\n",
        "    !wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "    !unzip trainingandtestdata.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7a2pzyDsUDY",
        "outputId": "fb4ef8c3-9fb7-4010-c74a-73d02b6e1e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File exists. Size: 227.74 MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Verify if file exists and check size\n",
        "file_path = 'training.1600000.processed.noemoticon.csv'\n",
        "if os.path.exists(file_path):\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    print(f\"File exists. Size: {file_size/1024/1024:.2f} MB\")\n",
        "    # Expected size should be around 80MB for the full dataset\n",
        "else:\n",
        "    print(\"File not found. Downloading...\")\n",
        "    !wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "    !unzip trainingandtestdata.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY72-LS5U6hF"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj8Ysp8n-XZd",
        "outputId": "1e5cca0c-881c-4247-ed09-7331b70437f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Successfully loaded 1600000 tweets.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File path\n",
        "file_path = '/content/training.1600000.processed.noemoticon.csv'\n",
        "\n",
        "# Define column names\n",
        "columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(\n",
        "        file_path,\n",
        "        encoding='latin-1',\n",
        "        names=columns,\n",
        "        usecols=['target', 'text'],\n",
        "        header=None,\n",
        "        on_bad_lines='skip'  # ‚úÖ Correct for pandas >=1.3.0\n",
        "        # nrows=100000        # Optional: limit rows for testing\n",
        "    )\n",
        "    print(f\"\\n‚úÖ Successfully loaded {len(df)} tweets.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Failed to load data: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly7AsyWy_P6C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ7Qk6hsAC5C",
        "outputId": "7c8db66d-8514-4419-c64c-071dd0680753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Final label distribution:\n",
            "label\n",
            "0    800000\n",
            "1    800000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Convert target labels safely\n",
        "def safe_convert(val):\n",
        "    try:\n",
        "        return int(val)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Apply conversion\n",
        "df['label'] = df['target'].apply(safe_convert)\n",
        "df = df.dropna(subset=['label'])  # Drop rows with invalid labels\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# Keep only binary labels: 0 (negative), 4 (positive)\n",
        "df = df[df['label'].isin([0, 4])]\n",
        "df['label'] = df['label'].map({0: 0, 4: 1})  # Map to binary\n",
        "\n",
        "# Check distribution\n",
        "print(\"‚úÖ Final label distribution:\")\n",
        "print(df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhiGrvJEAoz4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize once\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    # Lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove URLs, mentions, hashtags\n",
        "    text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', text)\n",
        "\n",
        "    # Remove punctuation, numbers, and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize and lemmatize\n",
        "    tokens = text.split()\n",
        "    cleaned = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(cleaned)\n",
        "\n",
        "# Apply to your dataframe\n",
        "df['clean_text'] = df['text'].apply(preprocess)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipPZL1X4AyUe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_text'],  # input features\n",
        "    df['label'],       # target labels\n",
        "    test_size=0.2,     # 80/20 split\n",
        "    random_state=42,   # reproducibility\n",
        "    stratify=df['label']  # preserve class distribution\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8Y6NJtWoA2f8"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=10000,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2)  # Unigrams + Bigrams\n",
        ")\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K7lIMYfaXvKM",
        "outputId": "3ed1b057-5212-48f5-a0f0-5dc5ddce565c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best SVM params: {'C': 0.1}\n",
            "Best SVM accuracy: 0.767827344114707\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.73      0.76    160000\n",
            "           1       0.75      0.80      0.78    160000\n",
            "\n",
            "    accuracy                           0.77    320000\n",
            "   macro avg       0.77      0.77      0.77    320000\n",
            "weighted avg       0.77      0.77      0.77    320000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
        "svm = LinearSVC(max_iter=10000)\n",
        "grid = GridSearchCV(svm, param_grid, cv=3, scoring='accuracy')\n",
        "grid.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"Best SVM params:\", grid.best_params_)\n",
        "print(\"Best SVM accuracy:\", grid.best_score_)\n",
        "\n",
        "y_pred = grid.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6D7wgSvBccOE",
        "outputId": "9dc64007-a3f1-4a0e-e680-b79ba8485cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Logistic Regression Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.74      0.76    160000\n",
            "           1       0.75      0.80      0.78    160000\n",
            "\n",
            "    accuracy                           0.77    320000\n",
            "   macro avg       0.77      0.77      0.77    320000\n",
            "weighted avg       0.77      0.77      0.77    320000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize the model\n",
        "logreg = LogisticRegression(\n",
        "    max_iter=200,       # Increase if convergence warnings appear\n",
        "    C=1.0,              # Regularization strength (can tune this)\n",
        "    solver='liblinear'  # Good for small datasets or L1 regularization\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = logreg.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate\n",
        "print(\"üìä Logistic Regression Performance:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MXyROCEO2J0I",
        "outputId": "5efa98b8-b633-46ec-9f4d-81f054c473dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Classification Report (Multinomial Naive Bayes):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.75      0.75      0.75    160000\n",
            "    positive       0.75      0.75      0.75    160000\n",
            "\n",
            "    accuracy                           0.75    320000\n",
            "   macro avg       0.75      0.75      0.75    320000\n",
            "weighted avg       0.75      0.75      0.75    320000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize and train the model\n",
        "model = MultinomialNB(alpha=0.5)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nüìä Classification Report (Multinomial Naive Bayes):\")\n",
        "print(classification_report(y_test, y_pred, target_names=['negative', 'positive']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kZvDBv3WhzoC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V271xDQmBHb6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aNbWJ6vcBM_a"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text):\n",
        "    try:\n",
        "        # Step 1: Preprocess the input\n",
        "        cleaned = preprocess(text)\n",
        "\n",
        "        if not cleaned.strip():\n",
        "            return {\n",
        "                \"text\": text,\n",
        "                \"error\": \"Text too short or invalid after preprocessing\"\n",
        "            }\n",
        "\n",
        "        # Step 2: Transform the cleaned text to TF-IDF features\n",
        "        vectorized = tfidf.transform([cleaned])\n",
        "\n",
        "        # Step 3: Make prediction\n",
        "        prediction = model.predict(vectorized)[0]\n",
        "\n",
        "        # Step 4: Predict probabilities (if supported by model)\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            proba = model.predict_proba(vectorized)[0]\n",
        "        else:\n",
        "            proba = [None, None]\n",
        "\n",
        "        # Step 5: Construct response\n",
        "        label_map = {0: \"negative\", 1: \"positive\"}\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"prediction\": label_map.get(prediction, \"unknown\"),\n",
        "            \"confidence\": {\n",
        "                \"negative\": f\"{(proba[0]*100):.1f}%\" if proba[0] is not None else \"N/A\",\n",
        "                \"positive\": f\"{(proba[1]*100):.1f}%\" if proba[1] is not None else \"N/A\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"text\": text, \"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pjesUMqmivFw",
        "outputId": "0ba62cad-1987-4c6e-d564-b82da1849897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Sentiment Analysis Test Results ===\n",
            "\n",
            "Text: 'I love this product! It's amazing.'\n",
            "Result: {'text': \"I love this product! It's amazing.\", 'prediction': 'positive', 'confidence': {'negative': '17.3%', 'positive': '82.7%'}}\n",
            "\n",
            "Text: 'This is terrible and awful.'\n",
            "Result: {'text': 'This is terrible and awful.', 'prediction': 'negative', 'confidence': {'negative': '94.3%', 'positive': '5.7%'}}\n",
            "\n",
            "Text: ''\n",
            "Result: {'text': '', 'error': 'Text too short or invalid after preprocessing'}\n",
            "\n",
            "Text: '123 456'\n",
            "Result: {'text': '123 456', 'error': 'Text too short or invalid after preprocessing'}\n",
            "\n",
            "Text: 'Very good'\n",
            "Result: {'text': 'Very good', 'prediction': 'positive', 'confidence': {'negative': '29.6%', 'positive': '70.4%'}}\n",
            "\n",
            "Text: 'I hate this so much!'\n",
            "Result: {'text': 'I hate this so much!', 'prediction': 'negative', 'confidence': {'negative': '87.9%', 'positive': '12.1%'}}\n",
            "\n",
            "Text: 'Not bad at all'\n",
            "Result: {'text': 'Not bad at all', 'prediction': 'negative', 'confidence': {'negative': '79.4%', 'positive': '20.6%'}}\n"
          ]
        }
      ],
      "source": [
        "# === Test Cases ===\n",
        "test_texts = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"This is terrible and awful.\",\n",
        "    \"\",\n",
        "    \"123 456\",\n",
        "    \"Very good\",\n",
        "    \"I hate this so much!\",\n",
        "    \"Not bad at all\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== Sentiment Analysis Test Results ===\")\n",
        "for text in test_texts:\n",
        "    result = predict_sentiment(text)\n",
        "    print(f\"\\nText: '{text}'\")\n",
        "    print(\"Result:\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iDttTMQqcINt",
        "outputId": "ddad8ba1-efad-41fa-81c2-fc67bb6fd5e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Save model and vectorizer\n",
        "joblib.dump(model, 'naive_bayes_model.pkl')\n",
        "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}